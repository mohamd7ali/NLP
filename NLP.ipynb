{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Dependencies**"
      ],
      "metadata": {
        "id": "l7sNjyVnNnQD"
      },
      "id": "l7sNjyVnNnQD"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchtext torchaudio"
      ],
      "metadata": {
        "id": "Mza76swhNj9P",
        "collapsed": true
      },
      "id": "Mza76swhNj9P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This Versions are not available & they are Removed!\"\"\"\n",
        "# !pip install torch==2.1.0+cu121 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install torchtext==0.16.0"
      ],
      "metadata": {
        "id": "LBNzz-u7Nlm3",
        "collapsed": true
      },
      "id": "LBNzz-u7Nlm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reinstall PyTorch and related libraries with CUDA 12.1 support.\n",
        "Note: Older versions (e.g., torch==2.1.0+cu121, torchtext==0.16.0) are no longer available.\n",
        "This cell installs the earliest available compatible versions:\n",
        "These versions are officially matched and stable to work together.\n",
        "\"\"\"\n",
        "\n",
        "# Install torch 2.2.0 + cu121 with compatible versions\n",
        "!pip install torch==2.2.0+cu121 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "# Install torchtext that matches torch 2.2.0\n",
        "!pip install torchtext==0.16.2"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI1gjbUM_ne6",
        "outputId": "b74443ec-5aa9-41ed-ae6a-2c416a578e41"
      },
      "id": "RI1gjbUM_ne6",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.2.0+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp312-cp312-linux_x86_64.whl (757.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.2/757.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu121) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu121) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu121) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu121) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu121) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (2.32.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0+cu121) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0+cu121) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0+cu121) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0+cu121 torchaudio-2.2.0+cu121 torchvision-0.17.0+cu121\n",
            "Collecting torchtext==0.16.2\n",
            "  Downloading torchtext-0.16.2-cp312-cp312-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.16.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.16.2) (2.32.4)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.12/dist-packages (from torchtext==0.16.2) (2.2.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.16.2) (1.26.4)\n",
            "Collecting torchdata==0.7.1 (from torchtext==0.16.2)\n",
            "  Downloading torchdata-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (12.1.105)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.7.1->torchtext==0.16.2) (2.5.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchtext==0.16.2) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.16.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.16.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.16.2) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0->torchtext==0.16.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0->torchtext==0.16.2) (1.3.0)\n",
            "Downloading torchtext-0.16.2-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.1-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchdata, torchtext\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.11.0\n",
            "    Uninstalling torchdata-0.11.0:\n",
            "      Successfully uninstalled torchdata-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchdata-0.7.1 torchtext-0.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2\""
      ],
      "metadata": {
        "id": "JUg_qBW80YKo"
      },
      "id": "JUg_qBW80YKo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchmetrics"
      ],
      "metadata": {
        "id": "y71sAkgdO9y6"
      },
      "id": "y71sAkgdO9y6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchdata"
      ],
      "metadata": {
        "id": "nNUKHxfnO_2Q"
      },
      "id": "nNUKHxfnO_2Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "94d6108c",
      "metadata": {
        "id": "94d6108c"
      },
      "source": [
        "---\n",
        "# **Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f80c2989",
      "metadata": {
        "id": "f80c2989"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Torchtext version:\", torchtext.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBTT35TPNtuU",
        "outputId": "93bf78d4-074e-4d85-e279-b89786550c81"
      },
      "id": "TBTT35TPNtuU",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.2.0+cu121\n",
            "Torchtext version: 0.16.2+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b98a5e3",
      "metadata": {
        "id": "2b98a5e3"
      },
      "source": [
        "---\n",
        "# **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce858f3",
      "metadata": {
        "id": "2ce858f3",
        "outputId": "de4a29a4-d7ac-4673-cd5e-7b6bb7bb09b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Aug 14 18:03:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   43C    P3             10W /   35W |       0MiB /   6144MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1b943b",
      "metadata": {
        "id": "1b1b943b"
      },
      "source": [
        "---\n",
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker\n",
        "\n",
        "# Restart the runtime (important, otherwise torchtext still sees None)\n",
        "import os, sys\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "WSdb_7gxbJvk"
      },
      "id": "WSdb_7gxbJvk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "abf195fa",
      "metadata": {
        "id": "abf195fa"
      },
      "outputs": [],
      "source": [
        "from torchtext import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = datasets.AG_NEWS(root='/content/', split=('train', 'test'))"
      ],
      "metadata": {
        "id": "9erS3WzmZuaA"
      },
      "id": "9erS3WzmZuaA",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gFj0wwZbo2K",
        "outputId": "6bf23217-eb85-40d8-e150-0acc9e14c110"
      },
      "id": "_gFj0wwZbo2K",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ShardingFilterIterDataPipe, ShardingFilterIterDataPipe)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **NLP Datasets:** ``` Iterable DataPipes```\n",
        "- **Computer Vision Datasets:** ```Map-Style DataPipes```"
      ],
      "metadata": {
        "id": "GyrErsBMcz-j"
      },
      "id": "GyrErsBMcz-j"
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcXs4sCQcHGy",
        "outputId": "deb208c4-ddc8-48ac-8af9-384c69fb0e1a"
      },
      "id": "PcXs4sCQcHGy",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = iter(train_set)\n",
        "print(train_iter)"
      ],
      "metadata": {
        "id": "_bHBbVbSiJH9",
        "outputId": "5adf52cc-8377-4f17-cf34-e4de7e5f5120",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_bHBbVbSiJH9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object ShardingFilterIterDataPipe.__iter__ at 0x7b38092c3740>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(train_iter)"
      ],
      "metadata": {
        "id": "AAU_OKmtjPgl",
        "outputId": "015e0040-6976-4a58-a888-5290f7a0d56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AAU_OKmtjPgl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(train_iter)"
      ],
      "metadata": {
        "id": "RSummZEzii7h",
        "outputId": "aa7b5946-42e7-4b6c-e5d2-efbf819e3559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RSummZEzii7h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Convert to Map-Style DataPipes/Dataset"
      ],
      "metadata": {
        "id": "l4jT9ZpkZizi"
      },
      "id": "l4jT9ZpkZizi"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.functional import to_map_style_dataset"
      ],
      "metadata": {
        "id": "Q7B8t-bPZjls"
      },
      "id": "Q7B8t-bPZjls",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the Dataset\n",
        "train_set_MapStyle, test_set_MapStyle = datasets.AG_NEWS(root='/content/', split=('train', 'test'))\n",
        "\n",
        "# Convert the iterable dataset to Map-Style Dataset\n",
        "train_set_MapStyle = to_map_style_dataset(train_set_MapStyle)\n",
        "test_set_MapStyle = to_map_style_dataset(train_set_MapStyle)"
      ],
      "metadata": {
        "id": "yAMu6UpjZmrW"
      },
      "id": "yAMu6UpjZmrW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(train_set_MapStyle[i])"
      ],
      "metadata": {
        "id": "_vmeEC29ZoDM",
        "outputId": "7001d434-d3ad-4e25-8c07-82ac13abd52c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_vmeEC29ZoDM",
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n",
            "(3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')\n",
            "(3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\")\n",
            "(3, 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.')\n",
            "(3, 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.')\n",
            "(3, 'Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\\\but stayed near lows for the year as oil prices surged past  #36;46\\\\a barrel, offsetting a positive outlook from computer maker\\\\Dell Inc. (DELL.O)')\n",
            "(3, \"Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\")\n",
            "(3, 'Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.')\n",
            "(3, 'Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.')\n",
            "(3, \"Wall St. Bears Claw Back Into the Black  NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **DataLoader**"
      ],
      "metadata": {
        "id": "0OpdfE5sr5vW"
      },
      "id": "0OpdfE5sr5vW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### BERT Tokenizer"
      ],
      "metadata": {
        "id": "X-7yhbBPC36T"
      },
      "id": "X-7yhbBPC36T"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.transforms import BERTTokenizer\n",
        "\n",
        "VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\"\n",
        "\n",
        "tokenizer_BERT = BERTTokenizer(vocab_path=VOCAB_FILE,\n",
        "                               do_lower_case=True,\n",
        "                               return_tokens=True)\n",
        "tokenizer_BERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5OxM9j0CIk7",
        "outputId": "7b95f8ae-aad2-4c07-c814-3a577e17a439"
      },
      "id": "s5OxM9j0CIk7",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 232k/232k [00:00<00:00, 5.36MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTTokenizer()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### GloVe Vectorization"
      ],
      "metadata": {
        "id": "1puCwp4qDbPH"
      },
      "id": "1puCwp4qDbPH"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe\n",
        "\n",
        "vectorization = GloVe(name=\"6B\", dim=100)\n",
        "vectorization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4dM1AAGDea7",
        "outputId": "603ba9c3-29e3-44b7-fc51-97ecb37741dc"
      },
      "id": "H4dM1AAGDea7",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:40, 2.15MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:26<00:00, 15090.82it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.vocab.vectors.GloVe at 0x7d8516c349e0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization.get_vecs_by_tokens(tokenizer_BERT('Hello World, How are you!')).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgl05p8fM_uT",
        "outputId": "854da37c-8e7d-411e-f56e-402225046c88"
      },
      "id": "wgl05p8fM_uT",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Bulid the DataLoader Structure"
      ],
      "metadata": {
        "id": "oDleMeNVC9d5"
      },
      "id": "oDleMeNVC9d5"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "CNytP2MSsCnD"
      },
      "id": "CNytP2MSsCnD",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *1st Output Try*"
      ],
      "metadata": {
        "id": "JI0v0pVDEK2J"
      },
      "id": "JI0v0pVDEK2J"
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batchs):\n",
        "    # extract & define labels in range of 0 to 3\n",
        "    labels = torch.LongTensor([batch[0] for batch in batchs]) - 1\n",
        "    # extract texts from the batch\n",
        "    texts = [batch[1] for batch in batchs]\n",
        "\n",
        "    return texts, labels\n"
      ],
      "metadata": {
        "id": "wnhZnOtA4Yqa"
      },
      "id": "wnhZnOtA4Yqa",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set, batch_size=2, shuffle=True, collate_fn=collate)\n",
        "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "eeD03rq13kbj"
      },
      "id": "eeD03rq13kbj",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-YgukQkBAbz",
        "outputId": "5129262e-bed1-4f47-a7bf-ff154af13fa2"
      },
      "id": "F-YgukQkBAbz",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['U.S. Signals Flexibility on Israeli Settlement Growth  CRAWFORD, Texas (Reuters) - The Bush administration  signaled on Saturday that it may accept limited growth within  existing Israeli settlements in the West Bank in a shift that  could help embattled Prime Minister Ariel Sharon.',\n",
              "  'Content software targets small publishers Snapbridge Software brings content management to the little guys.'],\n",
              " tensor([0, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *2nd Output Try - Final*"
      ],
      "metadata": {
        "id": "HvOAeQzoERvd"
      },
      "id": "HvOAeQzoERvd"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "RT7zpcBzSBgK"
      },
      "id": "RT7zpcBzSBgK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batchs):\n",
        "    # extract & define labels in range of 0 to 3\n",
        "    labels = torch.LongTensor([batch[0] for batch in batchs]) - 1\n",
        "    # extract texts from the batch\n",
        "    texts = [batch[1] for batch in batchs]\n",
        "\n",
        "    # tokenize the textx using BERT Tokenizer\n",
        "    tokens = tokenizer_BERT(texts)\n",
        "    num_tokens = len(tokens)\n",
        "    vectors = []\n",
        "\n",
        "    # Vectorization the tokens using GloVe\n",
        "    # vectors.append(vectorization.get_vecs_by_tokens(text_tokens) for text_tokens in tokens)\n",
        "    for i in range(num_tokens):\n",
        "\n",
        "        # vectorization of the tokens in batch\n",
        "        vectors.append(vectorization.get_vecs_by_tokens(tokens[i]))\n",
        "\n",
        "    # Pad the sequences to the maximum length in the batch\n",
        "    padded_vectors = pad_sequence(vectors, batch_first=False, padding_value=0.0)\n",
        "\n",
        "    return  padded_vectors, labels"
      ],
      "metadata": {
        "id": "7G7uKOqLEVMK"
      },
      "id": "7G7uKOqLEVMK",
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creat Train & Test Loader with ```collate```\n",
        "train_loader = DataLoader(train_set, batch_size=24, shuffle=True, collate_fn=collate)\n",
        "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, collate_fn=collate)"
      ],
      "metadata": {
        "id": "Ct_y8tH2WQ0v"
      },
      "id": "Ct_y8tH2WQ0v",
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ```Train Loader```"
      ],
      "metadata": {
        "id": "i1iRQ8_qXE-8"
      },
      "id": "i1iRQ8_qXE-8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the TrainLoader\n",
        "x, y = next(iter(train_loader))\n",
        "\n",
        "# Display the Reults\n",
        "print(\"--- Text Vectorization Results ---\")\n",
        "print(\"Shape: \", x.shape)\n",
        "print(x)\n",
        "\n",
        "print(80*\"=\")\n",
        "\n",
        "print(\"--- Labels ---\")\n",
        "print(y)\n",
        "print(\"Shape of Lables: \", y.shape)"
      ],
      "metadata": {
        "id": "UMKOCL3i3wOr",
        "outputId": "4bbbe196-8846-42e6-eb49-185c4bc23441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UMKOCL3i3wOr",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Text Vectorization Results ---\n",
            "Shape:  torch.Size([153, 24, 100])\n",
            "tensor([[[-0.3099, -0.1162,  0.2945,  ..., -0.0364,  0.2119, -0.2907],\n",
            "         [ 0.6621,  0.0665,  0.3640,  ..., -0.1493, -0.4993,  0.1637],\n",
            "         [-0.1659,  0.3444,  1.1095,  ...,  0.6920,  0.2801,  0.2384],\n",
            "         ...,\n",
            "         [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "         [-0.0311,  0.2216,  0.4449,  ..., -0.6710,  0.2470, -0.6200],\n",
            "         [-0.5261, -0.0670, -0.1735,  ..., -0.7912,  0.0476,  0.0844]],\n",
            "\n",
            "        [[-0.1786,  0.1492,  0.5122,  ...,  0.3148,  0.5245,  0.1552],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.0440,  0.1894,  0.6611,  ..., -0.1417,  0.9279,  0.5906],\n",
            "         [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "         [-1.2557,  0.6104,  0.5679,  ..., -1.5877,  0.7698, -0.6500]],\n",
            "\n",
            "        [[ 0.0587,  0.6981,  0.4263,  ..., -0.2222,  0.8061, -0.5056],\n",
            "         [-0.4028,  0.5850,  0.8757,  ...,  0.5656,  0.3707, -0.1148],\n",
            "         [-0.1077,  0.1105,  0.5981,  ..., -0.8316,  0.4529,  0.0826],\n",
            "         ...,\n",
            "         [ 0.3867,  0.6483,  0.7281,  ..., -0.4548,  0.3922,  0.5409],\n",
            "         [ 0.1374,  0.7789,  0.8005,  ..., -0.6168,  0.4470, -0.2797],\n",
            "         [ 0.2545, -0.5319,  0.2149,  ..., -0.4921,  0.8972, -0.3320]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
            "================================================================================\n",
            "--- Labels ---\n",
            "tensor([3, 0, 0, 0, 1, 3, 3, 2, 1, 2, 0, 1, 2, 2, 2, 3, 0, 3, 0, 1, 0, 3, 2, 3])\n",
            "Shape of Lables:  torch.Size([24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ```Test Loader```"
      ],
      "metadata": {
        "id": "FfYULWzZXIeU"
      },
      "id": "FfYULWzZXIeU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the Test Loader\n",
        "x, y = next(iter(test_loader))\n",
        "\n",
        "# Display the Reults\n",
        "print(\"--- Text Vectorization Results ---\")\n",
        "print(\"Shape: \", x.shape)\n",
        "print(x)\n",
        "\n",
        "print(80*\"=\")\n",
        "\n",
        "print(\"--- Labels ---\")\n",
        "print(y)\n",
        "print(\"Shape of Lables: \", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WEVbxpqWabg",
        "outputId": "ea54a21a-3f43-4d76-c31b-0559e1aec940"
      },
      "id": "7WEVbxpqWabg",
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Text Vectorization Results ---\n",
            "Shape:  torch.Size([198, 128, 100])\n",
            "tensor([[[ 0.1792,  0.3733,  0.5073,  ..., -0.0049, -0.0746,  0.0169],\n",
            "         [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "         [-0.9662, -0.0946,  0.9061,  ..., -1.2542,  0.8202, -0.2788],\n",
            "         ...,\n",
            "         [-0.1672,  0.0434,  1.1972,  ...,  0.0957,  1.0813, -0.1622],\n",
            "         [-0.5481, -0.5953,  0.5111,  ...,  0.0444,  0.3507,  0.5338],\n",
            "         [ 0.1110, -0.1874,  0.7070,  ...,  0.4406,  0.6769,  0.1460]],\n",
            "\n",
            "        [[-0.1440,  0.3255,  0.1426,  ...,  0.2540,  1.1078, -0.0731],\n",
            "         [ 0.4801,  0.1466,  0.7959,  ...,  0.3337,  1.0150,  0.0887],\n",
            "         [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.1889,  0.0375,  0.8074,  ...,  0.3299, -0.5725,  0.0508],\n",
            "         [ 0.6625,  0.0465,  0.6582,  ...,  0.0321,  0.7238,  0.3681]],\n",
            "\n",
            "        [[ 0.1348,  0.4022, -0.4227,  ..., -0.2799,  0.2894,  0.0438],\n",
            "         [-0.5426,  0.4148,  1.0322,  ..., -1.2969,  0.7622,  0.4635],\n",
            "         [ 0.6063, -0.3737, -0.3187,  ..., -0.3559,  1.0019,  0.1479],\n",
            "         ...,\n",
            "         [-0.5456,  1.0965,  1.5106,  ..., -0.9037,  0.4814,  0.0304],\n",
            "         [-0.3272,  0.0964,  0.3424,  ..., -0.3817,  0.4399,  0.2468],\n",
            "         [-0.1121,  0.7172,  0.2632,  ..., -0.1382, -0.1633,  0.0271]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
            "================================================================================\n",
            "--- Labels ---\n",
            "tensor([2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 3, 1, 1, 0, 3,\n",
            "        0, 1, 0, 1, 0, 3, 2, 3, 0, 0, 2, 2, 1, 1, 1, 3, 0, 3, 0, 0, 1, 0, 3, 3,\n",
            "        3, 0, 3, 1, 0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 2, 0, 0, 3, 0, 2, 3, 2, 1, 1,\n",
            "        1, 2, 0, 2, 1, 2, 3, 2, 0, 1, 0, 1, 0, 3, 2, 3, 3, 3, 3, 1, 3, 3, 2, 1,\n",
            "        0, 1, 3, 0, 3, 2, 1, 1])\n",
            "Shape of Lables:  torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Pre-Process**"
      ],
      "metadata": {
        "id": "__zm52ukXsWN"
      },
      "id": "__zm52ukXsWN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "L7JhKRH8ZnRF"
      },
      "id": "L7JhKRH8ZnRF"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "metadata": {
        "id": "m3H-n-zwZl9g"
      },
      "id": "m3H-n-zwZl9g",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"This is a test sentence.\"\n",
        "tokens = tokenizer(test_sentence)\n",
        "print(tokens)\n",
        "print(\"Number of tokens:\", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVS1LJtWZ-oS",
        "outputId": "fdf50cec-0850-4bcc-9887-085e011615bb"
      },
      "id": "eVS1LJtWZ-oS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'test', 'sentence', '.']\n",
            "Number of tokens: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = test_sentence.split()\n",
        "print(split)\n",
        "print(\"Number of split tokens:\", len(split))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS4dap6MbLgG",
        "outputId": "bc7150e3-f60f-431d-911b-9bb86ea8d993"
      },
      "id": "LS4dap6MbLgG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'test', 'sentence.']\n",
            "Number of split tokens: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the below Result shows, using the tokenizer ```basic_english``` can tokenize the text very well in comparison to command ```split``` that couldn't tokenize somthing like ., !, ?, ..."
      ],
      "metadata": {
        "id": "zsbXrTajbcYy"
      },
      "id": "zsbXrTajbcYy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Vectorization"
      ],
      "metadata": {
        "id": "-Eoel5meZyGR"
      },
      "id": "-Eoel5meZyGR"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "JJMAWRzwZ07U"
      },
      "id": "JJMAWRzwZ07U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GloVe"
      ],
      "metadata": {
        "id": "blQEh0DZfOwX"
      },
      "id": "blQEh0DZfOwX"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization = GloVe(name=\"6B\", dim=100)\n",
        "vectorization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPEzQYf9cI4P",
        "outputId": "76737289-8a9a-46bf-df5d-1813739ae4ce"
      },
      "id": "HPEzQYf9cI4P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.vocab.vectors.GloVe at 0x7a672874c490>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization.itos[:11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STRy30PQe5iE",
        "outputId": "a9e6eb58-8ecf-4780-f570-6e4442ecc12c"
      },
      "id": "STRy30PQe5iE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization.stoi[\"and\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0vP70Wbe9LY",
        "outputId": "5f72541e-c4df-43bb-c1ca-a7354c149372"
      },
      "id": "M0vP70Wbe9LY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization.stoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EV7K6PFFfNCl",
        "outputId": "df032ac5-c3b3-46e2-f0c4-7a075b79abc9"
      },
      "id": "EV7K6PFFfNCl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 0,\n",
              " ',': 1,\n",
              " '.': 2,\n",
              " 'of': 3,\n",
              " 'to': 4,\n",
              " 'and': 5,\n",
              " 'in': 6,\n",
              " 'a': 7,\n",
              " '\"': 8,\n",
              " \"'s\": 9,\n",
              " 'for': 10,\n",
              " '-': 11,\n",
              " 'that': 12,\n",
              " 'on': 13,\n",
              " 'is': 14,\n",
              " 'was': 15,\n",
              " 'said': 16,\n",
              " 'with': 17,\n",
              " 'he': 18,\n",
              " 'as': 19,\n",
              " 'it': 20,\n",
              " 'by': 21,\n",
              " 'at': 22,\n",
              " '(': 23,\n",
              " ')': 24,\n",
              " 'from': 25,\n",
              " 'his': 26,\n",
              " \"''\": 27,\n",
              " '``': 28,\n",
              " 'an': 29,\n",
              " 'be': 30,\n",
              " 'has': 31,\n",
              " 'are': 32,\n",
              " 'have': 33,\n",
              " 'but': 34,\n",
              " 'were': 35,\n",
              " 'not': 36,\n",
              " 'this': 37,\n",
              " 'who': 38,\n",
              " 'they': 39,\n",
              " 'had': 40,\n",
              " 'i': 41,\n",
              " 'which': 42,\n",
              " 'will': 43,\n",
              " 'their': 44,\n",
              " ':': 45,\n",
              " 'or': 46,\n",
              " 'its': 47,\n",
              " 'one': 48,\n",
              " 'after': 49,\n",
              " 'new': 50,\n",
              " 'been': 51,\n",
              " 'also': 52,\n",
              " 'we': 53,\n",
              " 'would': 54,\n",
              " 'two': 55,\n",
              " 'more': 56,\n",
              " \"'\": 57,\n",
              " 'first': 58,\n",
              " 'about': 59,\n",
              " 'up': 60,\n",
              " 'when': 61,\n",
              " 'year': 62,\n",
              " 'there': 63,\n",
              " 'all': 64,\n",
              " '--': 65,\n",
              " 'out': 66,\n",
              " 'she': 67,\n",
              " 'other': 68,\n",
              " 'people': 69,\n",
              " \"n't\": 70,\n",
              " 'her': 71,\n",
              " 'percent': 72,\n",
              " 'than': 73,\n",
              " 'over': 74,\n",
              " 'into': 75,\n",
              " 'last': 76,\n",
              " 'some': 77,\n",
              " 'government': 78,\n",
              " 'time': 79,\n",
              " '$': 80,\n",
              " 'you': 81,\n",
              " 'years': 82,\n",
              " 'if': 83,\n",
              " 'no': 84,\n",
              " 'world': 85,\n",
              " 'can': 86,\n",
              " 'three': 87,\n",
              " 'do': 88,\n",
              " ';': 89,\n",
              " 'president': 90,\n",
              " 'only': 91,\n",
              " 'state': 92,\n",
              " 'million': 93,\n",
              " 'could': 94,\n",
              " 'us': 95,\n",
              " 'most': 96,\n",
              " '_': 97,\n",
              " 'against': 98,\n",
              " 'u.s.': 99,\n",
              " 'so': 100,\n",
              " 'them': 101,\n",
              " 'what': 102,\n",
              " 'him': 103,\n",
              " 'united': 104,\n",
              " 'during': 105,\n",
              " 'before': 106,\n",
              " 'may': 107,\n",
              " 'since': 108,\n",
              " 'many': 109,\n",
              " 'while': 110,\n",
              " 'where': 111,\n",
              " 'states': 112,\n",
              " 'because': 113,\n",
              " 'now': 114,\n",
              " 'city': 115,\n",
              " 'made': 116,\n",
              " 'like': 117,\n",
              " 'between': 118,\n",
              " 'did': 119,\n",
              " 'just': 120,\n",
              " 'national': 121,\n",
              " 'day': 122,\n",
              " 'country': 123,\n",
              " 'under': 124,\n",
              " 'such': 125,\n",
              " 'second': 126,\n",
              " 'then': 127,\n",
              " 'company': 128,\n",
              " 'group': 129,\n",
              " 'any': 130,\n",
              " 'through': 131,\n",
              " 'china': 132,\n",
              " 'four': 133,\n",
              " 'being': 134,\n",
              " 'down': 135,\n",
              " 'war': 136,\n",
              " 'back': 137,\n",
              " 'off': 138,\n",
              " 'south': 139,\n",
              " 'american': 140,\n",
              " 'minister': 141,\n",
              " 'police': 142,\n",
              " 'well': 143,\n",
              " 'including': 144,\n",
              " 'team': 145,\n",
              " 'international': 146,\n",
              " 'week': 147,\n",
              " 'officials': 148,\n",
              " 'still': 149,\n",
              " 'both': 150,\n",
              " 'even': 151,\n",
              " 'high': 152,\n",
              " 'part': 153,\n",
              " 'told': 154,\n",
              " 'those': 155,\n",
              " 'end': 156,\n",
              " 'former': 157,\n",
              " 'these': 158,\n",
              " 'make': 159,\n",
              " 'billion': 160,\n",
              " 'work': 161,\n",
              " 'our': 162,\n",
              " 'home': 163,\n",
              " 'school': 164,\n",
              " 'party': 165,\n",
              " 'house': 166,\n",
              " 'old': 167,\n",
              " 'later': 168,\n",
              " 'get': 169,\n",
              " 'another': 170,\n",
              " 'tuesday': 171,\n",
              " 'news': 172,\n",
              " 'long': 173,\n",
              " 'five': 174,\n",
              " 'called': 175,\n",
              " '1': 176,\n",
              " 'wednesday': 177,\n",
              " 'military': 178,\n",
              " 'way': 179,\n",
              " 'used': 180,\n",
              " 'much': 181,\n",
              " 'next': 182,\n",
              " 'monday': 183,\n",
              " 'thursday': 184,\n",
              " 'friday': 185,\n",
              " 'game': 186,\n",
              " 'here': 187,\n",
              " '?': 188,\n",
              " 'should': 189,\n",
              " 'take': 190,\n",
              " 'very': 191,\n",
              " 'my': 192,\n",
              " 'north': 193,\n",
              " 'security': 194,\n",
              " 'season': 195,\n",
              " 'york': 196,\n",
              " 'how': 197,\n",
              " 'public': 198,\n",
              " 'early': 199,\n",
              " 'according': 200,\n",
              " 'several': 201,\n",
              " 'court': 202,\n",
              " 'say': 203,\n",
              " 'around': 204,\n",
              " 'foreign': 205,\n",
              " '10': 206,\n",
              " 'until': 207,\n",
              " 'set': 208,\n",
              " 'political': 209,\n",
              " 'says': 210,\n",
              " 'market': 211,\n",
              " 'however': 212,\n",
              " 'family': 213,\n",
              " 'life': 214,\n",
              " 'same': 215,\n",
              " 'general': 216,\n",
              " '–': 217,\n",
              " 'left': 218,\n",
              " 'good': 219,\n",
              " 'top': 220,\n",
              " 'university': 221,\n",
              " 'going': 222,\n",
              " 'number': 223,\n",
              " 'major': 224,\n",
              " 'known': 225,\n",
              " 'points': 226,\n",
              " 'won': 227,\n",
              " 'six': 228,\n",
              " 'month': 229,\n",
              " 'dollars': 230,\n",
              " 'bank': 231,\n",
              " '2': 232,\n",
              " 'iraq': 233,\n",
              " 'use': 234,\n",
              " 'members': 235,\n",
              " 'each': 236,\n",
              " 'area': 237,\n",
              " 'found': 238,\n",
              " 'official': 239,\n",
              " 'sunday': 240,\n",
              " 'place': 241,\n",
              " 'go': 242,\n",
              " 'based': 243,\n",
              " 'among': 244,\n",
              " 'third': 245,\n",
              " 'times': 246,\n",
              " 'took': 247,\n",
              " 'right': 248,\n",
              " 'days': 249,\n",
              " 'local': 250,\n",
              " 'economic': 251,\n",
              " 'countries': 252,\n",
              " 'see': 253,\n",
              " 'best': 254,\n",
              " 'report': 255,\n",
              " 'killed': 256,\n",
              " 'held': 257,\n",
              " 'business': 258,\n",
              " 'west': 259,\n",
              " 'does': 260,\n",
              " 'own': 261,\n",
              " '%': 262,\n",
              " 'came': 263,\n",
              " 'law': 264,\n",
              " 'months': 265,\n",
              " 'women': 266,\n",
              " \"'re\": 267,\n",
              " 'power': 268,\n",
              " 'think': 269,\n",
              " 'service': 270,\n",
              " 'children': 271,\n",
              " 'bush': 272,\n",
              " 'show': 273,\n",
              " '/': 274,\n",
              " 'help': 275,\n",
              " 'chief': 276,\n",
              " 'saturday': 277,\n",
              " 'system': 278,\n",
              " 'john': 279,\n",
              " 'support': 280,\n",
              " 'series': 281,\n",
              " 'play': 282,\n",
              " 'office': 283,\n",
              " 'following': 284,\n",
              " 'me': 285,\n",
              " 'meeting': 286,\n",
              " 'expected': 287,\n",
              " 'late': 288,\n",
              " 'washington': 289,\n",
              " 'games': 290,\n",
              " 'european': 291,\n",
              " 'league': 292,\n",
              " 'reported': 293,\n",
              " 'final': 294,\n",
              " 'added': 295,\n",
              " 'without': 296,\n",
              " 'british': 297,\n",
              " 'white': 298,\n",
              " 'history': 299,\n",
              " 'man': 300,\n",
              " 'men': 301,\n",
              " 'became': 302,\n",
              " 'want': 303,\n",
              " 'march': 304,\n",
              " 'case': 305,\n",
              " 'few': 306,\n",
              " 'run': 307,\n",
              " 'money': 308,\n",
              " 'began': 309,\n",
              " 'open': 310,\n",
              " 'name': 311,\n",
              " 'trade': 312,\n",
              " 'center': 313,\n",
              " '3': 314,\n",
              " 'israel': 315,\n",
              " 'oil': 316,\n",
              " 'too': 317,\n",
              " 'al': 318,\n",
              " 'film': 319,\n",
              " 'win': 320,\n",
              " 'led': 321,\n",
              " 'east': 322,\n",
              " 'central': 323,\n",
              " '20': 324,\n",
              " 'air': 325,\n",
              " 'come': 326,\n",
              " 'chinese': 327,\n",
              " 'town': 328,\n",
              " 'leader': 329,\n",
              " 'army': 330,\n",
              " 'line': 331,\n",
              " 'never': 332,\n",
              " 'little': 333,\n",
              " 'played': 334,\n",
              " 'prime': 335,\n",
              " 'death': 336,\n",
              " 'companies': 337,\n",
              " 'least': 338,\n",
              " 'put': 339,\n",
              " 'forces': 340,\n",
              " 'past': 341,\n",
              " 'de': 342,\n",
              " 'half': 343,\n",
              " 'june': 344,\n",
              " 'saying': 345,\n",
              " 'know': 346,\n",
              " 'federal': 347,\n",
              " 'french': 348,\n",
              " 'peace': 349,\n",
              " 'earlier': 350,\n",
              " 'capital': 351,\n",
              " 'force': 352,\n",
              " 'great': 353,\n",
              " 'union': 354,\n",
              " 'near': 355,\n",
              " 'released': 356,\n",
              " 'small': 357,\n",
              " 'department': 358,\n",
              " 'every': 359,\n",
              " 'health': 360,\n",
              " 'japan': 361,\n",
              " 'head': 362,\n",
              " 'ago': 363,\n",
              " 'night': 364,\n",
              " 'big': 365,\n",
              " 'cup': 366,\n",
              " 'election': 367,\n",
              " 'region': 368,\n",
              " 'director': 369,\n",
              " 'talks': 370,\n",
              " 'program': 371,\n",
              " 'far': 372,\n",
              " 'today': 373,\n",
              " 'statement': 374,\n",
              " 'july': 375,\n",
              " 'although': 376,\n",
              " 'district': 377,\n",
              " 'again': 378,\n",
              " 'born': 379,\n",
              " 'development': 380,\n",
              " 'leaders': 381,\n",
              " 'council': 382,\n",
              " 'close': 383,\n",
              " 'record': 384,\n",
              " 'along': 385,\n",
              " 'county': 386,\n",
              " 'france': 387,\n",
              " 'went': 388,\n",
              " 'point': 389,\n",
              " 'must': 390,\n",
              " 'spokesman': 391,\n",
              " 'your': 392,\n",
              " 'member': 393,\n",
              " 'plan': 394,\n",
              " 'financial': 395,\n",
              " 'april': 396,\n",
              " 'recent': 397,\n",
              " 'campaign': 398,\n",
              " 'become': 399,\n",
              " 'troops': 400,\n",
              " 'whether': 401,\n",
              " 'lost': 402,\n",
              " 'music': 403,\n",
              " '15': 404,\n",
              " 'got': 405,\n",
              " 'israeli': 406,\n",
              " '30': 407,\n",
              " 'need': 408,\n",
              " '4': 409,\n",
              " 'lead': 410,\n",
              " 'already': 411,\n",
              " 'russia': 412,\n",
              " 'though': 413,\n",
              " 'might': 414,\n",
              " 'free': 415,\n",
              " 'hit': 416,\n",
              " 'rights': 417,\n",
              " '11': 418,\n",
              " 'information': 419,\n",
              " 'away': 420,\n",
              " '12': 421,\n",
              " '5': 422,\n",
              " 'others': 423,\n",
              " 'control': 424,\n",
              " 'within': 425,\n",
              " 'large': 426,\n",
              " 'economy': 427,\n",
              " 'press': 428,\n",
              " 'agency': 429,\n",
              " 'water': 430,\n",
              " 'died': 431,\n",
              " 'career': 432,\n",
              " 'making': 433,\n",
              " '...': 434,\n",
              " 'deal': 435,\n",
              " 'attack': 436,\n",
              " 'side': 437,\n",
              " 'seven': 438,\n",
              " 'better': 439,\n",
              " 'less': 440,\n",
              " 'september': 441,\n",
              " 'once': 442,\n",
              " 'clinton': 443,\n",
              " 'main': 444,\n",
              " 'due': 445,\n",
              " 'committee': 446,\n",
              " 'building': 447,\n",
              " 'conference': 448,\n",
              " 'club': 449,\n",
              " 'january': 450,\n",
              " 'decision': 451,\n",
              " 'stock': 452,\n",
              " 'america': 453,\n",
              " 'given': 454,\n",
              " 'give': 455,\n",
              " 'often': 456,\n",
              " 'announced': 457,\n",
              " 'television': 458,\n",
              " 'industry': 459,\n",
              " 'order': 460,\n",
              " 'young': 461,\n",
              " \"'ve\": 462,\n",
              " 'palestinian': 463,\n",
              " 'age': 464,\n",
              " 'start': 465,\n",
              " 'administration': 466,\n",
              " 'russian': 467,\n",
              " 'prices': 468,\n",
              " 'round': 469,\n",
              " 'december': 470,\n",
              " 'nations': 471,\n",
              " \"'m\": 472,\n",
              " 'human': 473,\n",
              " 'india': 474,\n",
              " 'defense': 475,\n",
              " 'asked': 476,\n",
              " 'total': 477,\n",
              " 'october': 478,\n",
              " 'players': 479,\n",
              " 'bill': 480,\n",
              " 'important': 481,\n",
              " 'southern': 482,\n",
              " 'move': 483,\n",
              " 'fire': 484,\n",
              " 'population': 485,\n",
              " 'rose': 486,\n",
              " 'november': 487,\n",
              " 'include': 488,\n",
              " 'further': 489,\n",
              " 'nuclear': 490,\n",
              " 'street': 491,\n",
              " 'taken': 492,\n",
              " 'media': 493,\n",
              " 'different': 494,\n",
              " 'issue': 495,\n",
              " 'received': 496,\n",
              " 'secretary': 497,\n",
              " 'return': 498,\n",
              " 'college': 499,\n",
              " 'working': 500,\n",
              " 'community': 501,\n",
              " 'eight': 502,\n",
              " 'groups': 503,\n",
              " 'despite': 504,\n",
              " 'level': 505,\n",
              " 'largest': 506,\n",
              " 'whose': 507,\n",
              " 'attacks': 508,\n",
              " 'germany': 509,\n",
              " 'august': 510,\n",
              " 'change': 511,\n",
              " 'church': 512,\n",
              " 'nation': 513,\n",
              " 'german': 514,\n",
              " 'station': 515,\n",
              " 'london': 516,\n",
              " 'weeks': 517,\n",
              " 'having': 518,\n",
              " '18': 519,\n",
              " 'research': 520,\n",
              " 'black': 521,\n",
              " 'services': 522,\n",
              " 'story': 523,\n",
              " '6': 524,\n",
              " 'europe': 525,\n",
              " 'sales': 526,\n",
              " 'policy': 527,\n",
              " 'visit': 528,\n",
              " 'northern': 529,\n",
              " 'lot': 530,\n",
              " 'across': 531,\n",
              " 'per': 532,\n",
              " 'current': 533,\n",
              " 'board': 534,\n",
              " 'football': 535,\n",
              " 'ministry': 536,\n",
              " 'workers': 537,\n",
              " 'vote': 538,\n",
              " 'book': 539,\n",
              " 'fell': 540,\n",
              " 'seen': 541,\n",
              " 'role': 542,\n",
              " 'students': 543,\n",
              " 'shares': 544,\n",
              " 'iran': 545,\n",
              " 'process': 546,\n",
              " 'agreement': 547,\n",
              " 'quarter': 548,\n",
              " 'full': 549,\n",
              " 'match': 550,\n",
              " 'started': 551,\n",
              " 'growth': 552,\n",
              " 'yet': 553,\n",
              " 'moved': 554,\n",
              " 'possible': 555,\n",
              " 'western': 556,\n",
              " 'special': 557,\n",
              " '100': 558,\n",
              " 'plans': 559,\n",
              " 'interest': 560,\n",
              " 'behind': 561,\n",
              " 'strong': 562,\n",
              " 'england': 563,\n",
              " 'named': 564,\n",
              " 'food': 565,\n",
              " 'period': 566,\n",
              " 'real': 567,\n",
              " 'authorities': 568,\n",
              " 'car': 569,\n",
              " 'term': 570,\n",
              " 'rate': 571,\n",
              " 'race': 572,\n",
              " 'nearly': 573,\n",
              " 'korea': 574,\n",
              " 'enough': 575,\n",
              " 'site': 576,\n",
              " 'opposition': 577,\n",
              " 'keep': 578,\n",
              " '25': 579,\n",
              " 'call': 580,\n",
              " 'future': 581,\n",
              " 'taking': 582,\n",
              " 'island': 583,\n",
              " '2008': 584,\n",
              " '2006': 585,\n",
              " 'road': 586,\n",
              " 'outside': 587,\n",
              " 'really': 588,\n",
              " 'century': 589,\n",
              " 'democratic': 590,\n",
              " 'almost': 591,\n",
              " 'single': 592,\n",
              " 'share': 593,\n",
              " 'leading': 594,\n",
              " 'trying': 595,\n",
              " 'find': 596,\n",
              " 'album': 597,\n",
              " 'senior': 598,\n",
              " 'minutes': 599,\n",
              " 'together': 600,\n",
              " 'congress': 601,\n",
              " 'index': 602,\n",
              " 'australia': 603,\n",
              " 'results': 604,\n",
              " 'hard': 605,\n",
              " 'hours': 606,\n",
              " 'land': 607,\n",
              " 'action': 608,\n",
              " 'higher': 609,\n",
              " 'field': 610,\n",
              " 'cut': 611,\n",
              " 'coach': 612,\n",
              " 'elections': 613,\n",
              " 'san': 614,\n",
              " 'issues': 615,\n",
              " 'executive': 616,\n",
              " 'february': 617,\n",
              " 'production': 618,\n",
              " 'areas': 619,\n",
              " 'river': 620,\n",
              " 'face': 621,\n",
              " 'using': 622,\n",
              " 'japanese': 623,\n",
              " 'province': 624,\n",
              " 'park': 625,\n",
              " 'price': 626,\n",
              " 'commission': 627,\n",
              " 'california': 628,\n",
              " 'father': 629,\n",
              " 'son': 630,\n",
              " 'education': 631,\n",
              " '7': 632,\n",
              " 'village': 633,\n",
              " 'energy': 634,\n",
              " 'shot': 635,\n",
              " 'short': 636,\n",
              " 'africa': 637,\n",
              " 'key': 638,\n",
              " 'red': 639,\n",
              " 'association': 640,\n",
              " 'average': 641,\n",
              " 'pay': 642,\n",
              " 'exchange': 643,\n",
              " 'eu': 644,\n",
              " 'something': 645,\n",
              " 'gave': 646,\n",
              " 'likely': 647,\n",
              " 'player': 648,\n",
              " 'george': 649,\n",
              " '2007': 650,\n",
              " 'victory': 651,\n",
              " '8': 652,\n",
              " 'low': 653,\n",
              " 'things': 654,\n",
              " '2010': 655,\n",
              " 'pakistan': 656,\n",
              " '14': 657,\n",
              " 'post': 658,\n",
              " 'social': 659,\n",
              " 'continue': 660,\n",
              " 'ever': 661,\n",
              " 'look': 662,\n",
              " 'chairman': 663,\n",
              " 'job': 664,\n",
              " '2000': 665,\n",
              " 'soldiers': 666,\n",
              " 'able': 667,\n",
              " 'parliament': 668,\n",
              " 'front': 669,\n",
              " 'himself': 670,\n",
              " 'problems': 671,\n",
              " 'private': 672,\n",
              " 'lower': 673,\n",
              " 'list': 674,\n",
              " 'built': 675,\n",
              " '13': 676,\n",
              " 'efforts': 677,\n",
              " 'dollar': 678,\n",
              " 'miles': 679,\n",
              " 'included': 680,\n",
              " 'radio': 681,\n",
              " 'live': 682,\n",
              " 'form': 683,\n",
              " 'david': 684,\n",
              " 'african': 685,\n",
              " 'increase': 686,\n",
              " 'reports': 687,\n",
              " 'sent': 688,\n",
              " 'fourth': 689,\n",
              " 'always': 690,\n",
              " 'king': 691,\n",
              " '50': 692,\n",
              " 'tax': 693,\n",
              " 'taiwan': 694,\n",
              " 'britain': 695,\n",
              " '16': 696,\n",
              " 'playing': 697,\n",
              " 'title': 698,\n",
              " 'middle': 699,\n",
              " 'meet': 700,\n",
              " 'global': 701,\n",
              " 'wife': 702,\n",
              " '2009': 703,\n",
              " 'position': 704,\n",
              " 'located': 705,\n",
              " 'clear': 706,\n",
              " 'ahead': 707,\n",
              " '2004': 708,\n",
              " '2005': 709,\n",
              " 'iraqi': 710,\n",
              " 'english': 711,\n",
              " 'result': 712,\n",
              " 'release': 713,\n",
              " 'violence': 714,\n",
              " 'goal': 715,\n",
              " 'project': 716,\n",
              " 'closed': 717,\n",
              " 'border': 718,\n",
              " 'body': 719,\n",
              " 'soon': 720,\n",
              " 'crisis': 721,\n",
              " 'division': 722,\n",
              " '&amp;': 723,\n",
              " 'served': 724,\n",
              " 'tour': 725,\n",
              " 'hospital': 726,\n",
              " 'kong': 727,\n",
              " 'test': 728,\n",
              " 'hong': 729,\n",
              " 'u.n.': 730,\n",
              " 'inc.': 731,\n",
              " 'technology': 732,\n",
              " 'believe': 733,\n",
              " 'organization': 734,\n",
              " 'published': 735,\n",
              " 'weapons': 736,\n",
              " 'agreed': 737,\n",
              " 'why': 738,\n",
              " 'nine': 739,\n",
              " 'summer': 740,\n",
              " 'wanted': 741,\n",
              " 'republican': 742,\n",
              " 'act': 743,\n",
              " 'recently': 744,\n",
              " 'texas': 745,\n",
              " 'course': 746,\n",
              " 'problem': 747,\n",
              " 'senate': 748,\n",
              " 'medical': 749,\n",
              " 'un': 750,\n",
              " 'done': 751,\n",
              " 'reached': 752,\n",
              " 'star': 753,\n",
              " 'continued': 754,\n",
              " 'investors': 755,\n",
              " 'living': 756,\n",
              " 'care': 757,\n",
              " 'signed': 758,\n",
              " '17': 759,\n",
              " 'art': 760,\n",
              " 'provide': 761,\n",
              " 'worked': 762,\n",
              " 'presidential': 763,\n",
              " 'gold': 764,\n",
              " 'obama': 765,\n",
              " 'morning': 766,\n",
              " 'dead': 767,\n",
              " 'opened': 768,\n",
              " \"'ll\": 769,\n",
              " 'event': 770,\n",
              " 'previous': 771,\n",
              " 'cost': 772,\n",
              " 'instead': 773,\n",
              " 'canada': 774,\n",
              " 'band': 775,\n",
              " 'teams': 776,\n",
              " 'daily': 777,\n",
              " '2001': 778,\n",
              " 'available': 779,\n",
              " 'drug': 780,\n",
              " 'coming': 781,\n",
              " '2003': 782,\n",
              " 'investment': 783,\n",
              " '’s': 784,\n",
              " 'michael': 785,\n",
              " 'civil': 786,\n",
              " 'woman': 787,\n",
              " 'training': 788,\n",
              " 'appeared': 789,\n",
              " '9': 790,\n",
              " 'involved': 791,\n",
              " 'indian': 792,\n",
              " 'similar': 793,\n",
              " 'situation': 794,\n",
              " '24': 795,\n",
              " 'los': 796,\n",
              " 'running': 797,\n",
              " 'fighting': 798,\n",
              " 'mark': 799,\n",
              " '40': 800,\n",
              " 'trial': 801,\n",
              " 'hold': 802,\n",
              " 'australian': 803,\n",
              " 'thought': 804,\n",
              " '!': 805,\n",
              " 'study': 806,\n",
              " 'fall': 807,\n",
              " 'mother': 808,\n",
              " 'met': 809,\n",
              " 'relations': 810,\n",
              " 'anti': 811,\n",
              " '2002': 812,\n",
              " 'song': 813,\n",
              " 'popular': 814,\n",
              " 'base': 815,\n",
              " 'tv': 816,\n",
              " 'ground': 817,\n",
              " 'markets': 818,\n",
              " 'ii': 819,\n",
              " 'newspaper': 820,\n",
              " 'staff': 821,\n",
              " 'saw': 822,\n",
              " 'hand': 823,\n",
              " 'hope': 824,\n",
              " 'operations': 825,\n",
              " 'pressure': 826,\n",
              " 'americans': 827,\n",
              " 'eastern': 828,\n",
              " 'st.': 829,\n",
              " 'legal': 830,\n",
              " 'asia': 831,\n",
              " 'budget': 832,\n",
              " 'returned': 833,\n",
              " 'considered': 834,\n",
              " 'love': 835,\n",
              " 'wrote': 836,\n",
              " 'stop': 837,\n",
              " 'fight': 838,\n",
              " 'currently': 839,\n",
              " 'charges': 840,\n",
              " 'try': 841,\n",
              " 'aid': 842,\n",
              " 'ended': 843,\n",
              " 'management': 844,\n",
              " 'brought': 845,\n",
              " 'cases': 846,\n",
              " 'decided': 847,\n",
              " 'failed': 848,\n",
              " 'network': 849,\n",
              " 'works': 850,\n",
              " 'gas': 851,\n",
              " 'turned': 852,\n",
              " 'fact': 853,\n",
              " 'vice': 854,\n",
              " 'ca': 855,\n",
              " 'mexico': 856,\n",
              " 'trading': 857,\n",
              " 'especially': 858,\n",
              " 'reporters': 859,\n",
              " 'afghanistan': 860,\n",
              " 'common': 861,\n",
              " 'looking': 862,\n",
              " 'space': 863,\n",
              " 'rates': 864,\n",
              " 'manager': 865,\n",
              " 'loss': 866,\n",
              " '2011': 867,\n",
              " 'justice': 868,\n",
              " 'thousands': 869,\n",
              " 'james': 870,\n",
              " 'rather': 871,\n",
              " 'fund': 872,\n",
              " 'thing': 873,\n",
              " 'republic': 874,\n",
              " 'opening': 875,\n",
              " 'accused': 876,\n",
              " 'winning': 877,\n",
              " 'scored': 878,\n",
              " 'championship': 879,\n",
              " 'example': 880,\n",
              " 'getting': 881,\n",
              " 'biggest': 882,\n",
              " 'performance': 883,\n",
              " 'sports': 884,\n",
              " '1998': 885,\n",
              " 'let': 886,\n",
              " 'allowed': 887,\n",
              " 'schools': 888,\n",
              " 'means': 889,\n",
              " 'turn': 890,\n",
              " 'leave': 891,\n",
              " 'no.': 892,\n",
              " 'robert': 893,\n",
              " 'personal': 894,\n",
              " 'stocks': 895,\n",
              " 'showed': 896,\n",
              " 'light': 897,\n",
              " 'arrested': 898,\n",
              " 'person': 899,\n",
              " 'either': 900,\n",
              " 'offer': 901,\n",
              " 'majority': 902,\n",
              " 'battle': 903,\n",
              " '19': 904,\n",
              " 'class': 905,\n",
              " 'evidence': 906,\n",
              " 'makes': 907,\n",
              " 'society': 908,\n",
              " 'products': 909,\n",
              " 'regional': 910,\n",
              " 'needed': 911,\n",
              " 'stage': 912,\n",
              " 'am': 913,\n",
              " 'doing': 914,\n",
              " 'families': 915,\n",
              " 'construction': 916,\n",
              " 'various': 917,\n",
              " '1996': 918,\n",
              " 'sold': 919,\n",
              " 'independent': 920,\n",
              " 'kind': 921,\n",
              " 'airport': 922,\n",
              " 'paul': 923,\n",
              " 'judge': 924,\n",
              " 'internet': 925,\n",
              " 'movement': 926,\n",
              " 'room': 927,\n",
              " 'followed': 928,\n",
              " 'original': 929,\n",
              " 'angeles': 930,\n",
              " 'italy': 931,\n",
              " '`': 932,\n",
              " 'data': 933,\n",
              " 'comes': 934,\n",
              " 'parties': 935,\n",
              " 'nothing': 936,\n",
              " 'sea': 937,\n",
              " 'bring': 938,\n",
              " '2012': 939,\n",
              " 'annual': 940,\n",
              " 'officer': 941,\n",
              " 'beijing': 942,\n",
              " 'present': 943,\n",
              " 'remain': 944,\n",
              " 'nato': 945,\n",
              " '1999': 946,\n",
              " '22': 947,\n",
              " 'remains': 948,\n",
              " 'allow': 949,\n",
              " 'florida': 950,\n",
              " 'computer': 951,\n",
              " '21': 952,\n",
              " 'contract': 953,\n",
              " 'coast': 954,\n",
              " 'created': 955,\n",
              " 'demand': 956,\n",
              " 'operation': 957,\n",
              " 'events': 958,\n",
              " 'islamic': 959,\n",
              " 'beat': 960,\n",
              " 'analysts': 961,\n",
              " 'interview': 962,\n",
              " 'helped': 963,\n",
              " 'child': 964,\n",
              " 'probably': 965,\n",
              " 'spent': 966,\n",
              " 'asian': 967,\n",
              " 'effort': 968,\n",
              " 'cooperation': 969,\n",
              " 'shows': 970,\n",
              " 'calls': 971,\n",
              " 'investigation': 972,\n",
              " 'lives': 973,\n",
              " 'video': 974,\n",
              " 'yen': 975,\n",
              " 'runs': 976,\n",
              " 'tried': 977,\n",
              " 'bad': 978,\n",
              " 'described': 979,\n",
              " '1994': 980,\n",
              " 'toward': 981,\n",
              " 'written': 982,\n",
              " 'throughout': 983,\n",
              " 'established': 984,\n",
              " 'mission': 985,\n",
              " 'associated': 986,\n",
              " 'buy': 987,\n",
              " 'growing': 988,\n",
              " 'green': 989,\n",
              " 'forward': 990,\n",
              " 'competition': 991,\n",
              " 'poor': 992,\n",
              " 'latest': 993,\n",
              " 'banks': 994,\n",
              " 'question': 995,\n",
              " '1997': 996,\n",
              " 'prison': 997,\n",
              " 'feel': 998,\n",
              " 'attention': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Giving Tokens to GloVe\n"
      ],
      "metadata": {
        "id": "iqNeEyR4fS4g"
      },
      "id": "iqNeEyR4fS4g"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization.get_vecs_by_tokens(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hMlxbPGkcgeW",
        "outputId": "547c4e0b-2c1c-4d07-944a-c5b851ef0eeb"
      },
      "id": "hMlxbPGkcgeW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.7058e-01,  4.4183e-01,  7.0102e-01, -4.1713e-01, -3.4058e-01,\n",
              "          2.3390e-02, -7.1537e-02,  4.8177e-01, -1.3121e-02,  1.6834e-01,\n",
              "         -1.3389e-01,  4.0626e-02,  1.5827e-01, -4.4342e-01, -1.9403e-02,\n",
              "         -9.6610e-03, -4.6284e-02,  9.3228e-02, -2.7331e-01,  2.2850e-01,\n",
              "          3.3089e-01, -3.6474e-01,  7.8741e-02,  3.5850e-01,  4.4757e-01,\n",
              "         -2.2990e-01,  1.8077e-01, -6.2650e-01,  5.3852e-02, -2.9154e-01,\n",
              "         -4.2560e-01,  6.2903e-01,  1.4393e-01, -4.6004e-02, -2.1007e-01,\n",
              "          4.8879e-01, -5.7698e-02,  3.7431e-01, -3.0075e-02, -3.4494e-01,\n",
              "         -2.9702e-01,  1.5095e-01,  2.8248e-01, -1.6578e-01,  7.6131e-02,\n",
              "         -9.3016e-02,  7.9365e-01, -6.0489e-01, -1.8874e-01, -1.0173e+00,\n",
              "          3.1962e-01, -1.6344e-01,  5.4177e-01,  1.1725e+00, -4.7875e-01,\n",
              "         -3.3842e+00, -8.1301e-02, -3.5280e-01,  1.8372e+00,  4.4516e-01,\n",
              "         -5.2666e-01,  9.9786e-01, -3.2178e-01,  3.3462e-02,  1.1783e+00,\n",
              "         -7.2905e-02,  3.9737e-01,  2.6166e-01,  3.3111e-01, -3.5629e-01,\n",
              "         -1.6558e-01, -4.4382e-01, -1.4183e-01, -3.7976e-01,  2.8994e-01,\n",
              "         -2.9114e-02, -3.5169e-01, -2.7694e-01, -1.3440e+00,  1.9555e-01,\n",
              "          1.6887e-01,  4.0237e-02, -8.0212e-01,  2.3366e-01, -1.3837e+00,\n",
              "         -2.3132e-02,  8.5395e-02, -7.4051e-01, -7.3934e-02, -5.8838e-01,\n",
              "         -8.5735e-02, -1.0525e-01, -5.1571e-01,  1.5038e-01, -1.6694e-01,\n",
              "         -1.6372e-01, -2.2702e-01, -6.6102e-01,  4.7197e-01,  3.7253e-01],\n",
              "        [-5.4264e-01,  4.1476e-01,  1.0322e+00, -4.0244e-01,  4.6691e-01,\n",
              "          2.1816e-01, -7.4864e-02,  4.7332e-01,  8.0996e-02, -2.2079e-01,\n",
              "         -1.2808e-01, -1.1440e-01,  5.0891e-01,  1.1568e-01,  2.8211e-02,\n",
              "         -3.6280e-01,  4.3823e-01,  4.7511e-02,  2.0282e-01,  4.9857e-01,\n",
              "         -1.0068e-01,  1.3269e-01,  1.6972e-01,  1.1653e-01,  3.1355e-01,\n",
              "          2.5713e-01,  9.2783e-02, -5.6826e-01, -5.2975e-01, -5.1456e-02,\n",
              "         -6.7326e-01,  9.2533e-01,  2.6930e-01,  2.2734e-01,  6.6365e-01,\n",
              "          2.6221e-01,  1.9719e-01,  2.6090e-01,  1.8774e-01, -3.4540e-01,\n",
              "         -4.2635e-01,  1.3975e-01,  5.6338e-01, -5.6907e-01,  1.2398e-01,\n",
              "         -1.2894e-01,  7.2484e-01, -2.6105e-01, -2.6314e-01, -4.3605e-01,\n",
              "          7.8908e-02, -8.4146e-01,  5.1595e-01,  1.3997e+00, -7.6460e-01,\n",
              "         -3.1453e+00, -2.9202e-01, -3.1247e-01,  1.5129e+00,  5.2435e-01,\n",
              "          2.1456e-01,  4.2452e-01, -8.8411e-02, -1.7805e-01,  1.1876e+00,\n",
              "          1.0579e-01,  7.6571e-01,  2.1914e-01,  3.5824e-01, -1.1636e-01,\n",
              "          9.3261e-02, -6.2483e-01, -2.1898e-01,  2.1796e-01,  7.4056e-01,\n",
              "         -4.3735e-01,  1.4343e-01,  1.4719e-01, -1.1605e+00, -5.0508e-02,\n",
              "          1.2677e-01, -1.4395e-02, -9.8676e-01, -9.1297e-02, -1.2054e+00,\n",
              "         -1.1974e-01,  4.7847e-02, -5.4001e-01,  5.2457e-01, -7.0963e-01,\n",
              "         -3.2528e-01, -1.3460e-01, -4.1314e-01,  3.3435e-01, -7.2412e-03,\n",
              "          3.2253e-01, -4.4219e-02, -1.2969e+00,  7.6217e-01,  4.6349e-01],\n",
              "        [-2.7086e-01,  4.4006e-02, -2.0260e-02, -1.7395e-01,  6.4440e-01,\n",
              "          7.1213e-01,  3.5510e-01,  4.7138e-01, -2.9637e-01,  5.4427e-01,\n",
              "         -7.2294e-01, -4.7612e-03,  4.0611e-02,  4.3236e-02,  2.9729e-01,\n",
              "          1.0725e-01,  4.0156e-01, -5.3662e-01,  3.3382e-02,  6.7396e-02,\n",
              "          6.4556e-01, -8.5523e-02,  1.4103e-01,  9.4539e-02,  7.4947e-01,\n",
              "         -1.9400e-01, -6.8739e-01, -4.1741e-01, -2.2807e-01,  1.2000e-01,\n",
              "         -4.8999e-01,  8.0945e-01,  4.5138e-02, -1.1898e-01,  2.0161e-01,\n",
              "          3.9276e-01, -2.0121e-01,  3.1354e-01,  7.5304e-01,  2.5907e-01,\n",
              "         -1.1566e-01, -2.9319e-02,  9.3499e-01, -3.6067e-01,  5.2420e-01,\n",
              "          2.3706e-01,  5.2715e-01,  2.2869e-01, -5.1958e-01, -7.9349e-01,\n",
              "         -2.0368e-01, -5.0187e-01,  1.8748e-01,  9.4282e-01, -4.4834e-01,\n",
              "         -3.6792e+00,  4.4183e-02, -2.6751e-01,  2.1997e+00,  2.4100e-01,\n",
              "         -3.3425e-02,  6.9553e-01, -6.4472e-01, -7.2277e-03,  8.9575e-01,\n",
              "          2.0015e-01,  4.6493e-01,  6.1933e-01, -1.0660e-01,  8.6910e-02,\n",
              "         -4.6230e-01,  1.8262e-01, -1.5849e-01,  2.0791e-02,  1.9373e-01,\n",
              "          6.3426e-02, -3.1673e-01, -4.8177e-01, -1.3848e+00,  1.3669e-01,\n",
              "          9.6859e-01,  4.9965e-02, -2.7380e-01, -3.5686e-02, -1.0577e+00,\n",
              "         -2.4467e-01,  9.0366e-01, -1.2442e-01,  8.0776e-02, -8.3401e-01,\n",
              "          5.7201e-01,  8.8945e-02, -4.2532e-01, -1.8253e-02, -7.9995e-02,\n",
              "         -2.8581e-01, -1.0890e-02, -4.9230e-01,  6.3687e-01,  2.3642e-01],\n",
              "        [-5.8342e-01,  4.8631e-01,  7.4230e-01,  1.7875e-01, -1.5873e+00,\n",
              "         -3.7499e-01,  3.7902e-01,  7.0767e-01, -1.5402e+00,  7.4851e-01,\n",
              "         -6.9018e-03, -4.4981e-01, -1.1125e-01,  1.7395e-01,  2.6005e-01,\n",
              "          2.7065e-01,  7.8731e-01,  8.4877e-01, -9.9277e-02,  1.7688e-02,\n",
              "          4.4389e-01, -8.7300e-01,  4.9640e-01, -1.8430e-01,  2.4894e-01,\n",
              "          2.4073e-01,  8.4460e-02,  7.0786e-02, -2.3216e-01,  2.6685e-01,\n",
              "         -2.3519e-01,  4.5211e-01, -3.9982e-01,  2.4936e-01,  7.3548e-01,\n",
              "         -7.2352e-02, -8.1008e-01, -1.5256e-01, -1.0313e+00,  2.3067e-01,\n",
              "         -1.1634e+00,  2.0387e-01,  4.2369e-01, -1.0589e+00,  2.9905e-01,\n",
              "          2.0036e-01,  6.3371e-01, -5.7507e-01, -4.3730e-01, -5.5908e-01,\n",
              "          5.0811e-01,  3.2673e-01, -4.3710e-01,  1.0702e+00, -1.0418e-01,\n",
              "         -2.4432e+00, -1.0739e+00,  1.0881e-01,  1.5446e+00, -1.5633e-01,\n",
              "         -3.7779e-01,  1.5111e-01,  5.8397e-01,  5.4980e-01,  2.3775e-01,\n",
              "          8.8690e-01, -1.0220e-01, -4.1313e-02, -1.9496e-01, -1.3760e-01,\n",
              "         -1.3191e-04,  6.9490e-01,  6.6230e-02, -2.5647e-01, -1.0539e-01,\n",
              "          5.6950e-01,  1.7414e-01, -5.9899e-01, -8.6757e-01, -3.1254e-01,\n",
              "          5.4560e-01, -9.2359e-02, -1.2294e-01, -1.9419e-01, -1.9168e+00,\n",
              "         -4.4395e-02,  6.8561e-01,  2.0030e-01, -3.7791e-01,  6.7436e-01,\n",
              "         -7.7218e-01,  4.9596e-01,  5.8190e-03,  5.3833e-01,  3.3367e-01,\n",
              "          9.7822e-01,  3.1984e-01, -1.2619e-01, -1.7724e-02,  1.8389e-01],\n",
              "        [ 6.1920e-01,  1.4650e-01, -8.5925e-02, -2.6298e-01,  7.8439e-01,\n",
              "          8.8508e-01, -3.2910e-01,  4.9896e-01, -7.9062e-01,  1.0131e+00,\n",
              "         -8.8145e-01,  1.3584e+00, -6.6751e-01,  4.2515e-01,  5.0281e-01,\n",
              "         -8.9359e-02, -4.9863e-01, -8.3455e-01, -7.4469e-01,  3.4275e-01,\n",
              "          2.9674e-01, -7.1928e-01, -1.6707e-01,  4.0454e-01,  3.8765e-01,\n",
              "          6.2356e-01, -7.9742e-02, -8.4977e-01, -4.1216e-02,  7.3005e-01,\n",
              "          6.0823e-01, -1.6954e-02, -3.5062e-01, -7.6393e-01, -2.7945e-01,\n",
              "          3.5849e-01,  1.2919e-01, -6.3486e-01,  1.0818e-02, -2.5926e-01,\n",
              "         -3.5931e-01, -3.2015e-01,  1.2141e+00, -2.5545e-01,  3.5797e-01,\n",
              "          1.6637e-01,  1.0474e+00, -1.3668e+00, -1.1174e-01, -1.3239e-01,\n",
              "          9.8993e-01, -1.1606e+00,  5.6178e-01,  1.4242e+00, -7.0644e-01,\n",
              "         -5.1683e-01, -8.7395e-02, -6.2826e-01,  1.7740e+00,  5.8555e-01,\n",
              "         -1.3339e-01, -1.5930e-01, -7.4275e-01, -1.0585e+00,  9.1011e-01,\n",
              "         -4.9966e-01,  3.9680e-01,  6.2120e-01, -1.4312e+00,  3.3652e-01,\n",
              "          2.2723e-01,  1.3312e-01,  4.9876e-01,  3.4991e-01,  3.2350e-01,\n",
              "          7.3083e-01, -9.4294e-02, -8.7917e-01, -4.0376e-01, -4.0543e-01,\n",
              "          5.1081e-01, -5.1537e-01, -5.7041e-01, -8.1079e-01, -1.5331e+00,\n",
              "          1.2919e-01,  8.8052e-02, -7.3564e-01,  9.6530e-02, -3.5566e-01,\n",
              "         -2.5717e-01, -3.3758e-01, -2.4789e-01, -2.4340e-01,  8.8116e-01,\n",
              "          1.1735e-01,  5.6470e-01, -2.5014e-01, -1.4849e-01, -3.0727e-01],\n",
              "        [-3.3979e-01,  2.0941e-01,  4.6348e-01, -6.4792e-01, -3.8377e-01,\n",
              "          3.8034e-02,  1.7127e-01,  1.5978e-01,  4.6619e-01, -1.9169e-02,\n",
              "          4.1479e-01, -3.4349e-01,  2.6872e-01,  4.4640e-02,  4.2131e-01,\n",
              "         -4.1032e-01,  1.5459e-01,  2.2239e-02, -6.4653e-01,  2.5256e-01,\n",
              "          4.3136e-02, -1.9445e-01,  4.6516e-01,  4.5651e-01,  6.8588e-01,\n",
              "          9.1295e-02,  2.1875e-01, -7.0351e-01,  1.6785e-01, -3.5079e-01,\n",
              "         -1.2634e-01,  6.6384e-01, -2.5820e-01,  3.6542e-02, -1.3605e-01,\n",
              "          4.0253e-01,  1.4289e-01,  3.8132e-01, -1.2283e-01, -4.5886e-01,\n",
              "         -2.5282e-01, -3.0432e-01, -1.1215e-01, -2.6182e-01, -2.2482e-01,\n",
              "         -4.4554e-01,  2.9910e-01, -8.5612e-01, -1.4503e-01, -4.9086e-01,\n",
              "          8.2973e-03, -1.7491e-01,  2.7524e-01,  1.4401e+00, -2.1239e-01,\n",
              "         -2.8435e+00, -2.7958e-01, -4.5722e-01,  1.6386e+00,  7.8808e-01,\n",
              "         -5.5262e-01,  6.5000e-01,  8.6426e-02,  3.9012e-01,  1.0632e+00,\n",
              "         -3.5379e-01,  4.8328e-01,  3.4600e-01,  8.4174e-01,  9.8707e-02,\n",
              "         -2.4213e-01, -2.7053e-01,  4.5287e-02, -4.0147e-01,  1.1395e-01,\n",
              "          6.2226e-03,  3.6673e-02,  1.8518e-02, -1.0213e+00, -2.0806e-01,\n",
              "          6.4072e-01, -6.8763e-02, -5.8635e-01,  3.3476e-01, -1.1432e+00,\n",
              "         -1.1480e-01, -2.5091e-01, -4.5907e-01, -9.6819e-02, -1.7946e-01,\n",
              "         -6.3351e-02, -6.7412e-01, -6.8895e-02,  5.3604e-01, -8.7773e-01,\n",
              "          3.1802e-01, -3.9242e-01, -2.3394e-01,  4.7298e-01, -2.8803e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization.get_vecs_by_tokens(tokens).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bKOa3cfdci8",
        "outputId": "94487fef-721e-4abd-a97e-40b210c310e0"
      },
      "id": "6bKOa3cfdci8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we had a sentence of \"This is a test sentence.\" that the tokenizer, tokenize it to 6 token. Also we choose the \"dim=100\" for the GloVe.\n",
        "\n",
        "So the output correctly shows the sape of vectorization [6, 100].\n"
      ],
      "metadata": {
        "id": "oqQI2Li8dhgY"
      },
      "id": "oqQI2Li8dhgY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the Vectorization Results of different Words"
      ],
      "metadata": {
        "id": "Uboc0cU9f4J8"
      },
      "id": "Uboc0cU9f4J8"
    },
    {
      "cell_type": "code",
      "source": [
        "vec_boy = vectorization.get_vecs_by_tokens('boy')\n",
        "vec_girl = vectorization.get_vecs_by_tokens('girl')\n",
        "vec_street = vectorization.get_vecs_by_tokens('street')"
      ],
      "metadata": {
        "id": "sP7ZM2_kgCSR"
      },
      "id": "sP7ZM2_kgCSR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "print(\"--- Cosine Similarity ---\")\n",
        "print(\"boy VS. girl:\", F.cosine_similarity(vec_boy, vec_girl, dim=0))\n",
        "print(\"boy VS. street:\", F.cosine_similarity(vec_boy, vec_street, dim=0))\n",
        "print(\"girl VS. street:\", F.cosine_similarity(vec_girl, vec_street, dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTNaMGU7gQbr",
        "outputId": "f3290dc2-6722-4350-f7c2-7e019f6a6735"
      },
      "id": "qTNaMGU7gQbr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Cosine Similarity ---\n",
            "boy VS. girl: tensor(0.9176)\n",
            "boy VS. street: tensor(0.3859)\n",
            "girl VS. street: tensor(0.3354)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the results show, the vectorization outputs are correct because the `boy` and the `girl` are more similar to each other compared to the `street`, since both are human/family-related."
      ],
      "metadata": {
        "id": "_aJLhbS9hP3f"
      },
      "id": "_aJLhbS9hP3f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Transform ( Optional - just for Test )"
      ],
      "metadata": {
        "id": "jZW7X2U2cxRk"
      },
      "id": "jZW7X2U2cxRk"
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext.transforms as T"
      ],
      "metadata": {
        "id": "CqpfBnfbcz2b"
      },
      "id": "CqpfBnfbcz2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### BERT Tokenizer"
      ],
      "metadata": {
        "id": "9axz2UJpoJl5"
      },
      "id": "9axz2UJpoJl5"
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\"\n",
        "\n",
        "tokenizer_BERT = T.BERTTokenizer(vocab_path=VOCAB_FILE,\n",
        "                            do_lower_case=True)\n",
        "\n",
        "print(\"\\n\",tokenizer_BERT('Hello World, How are you!'))\n",
        "print(tokenizer_BERT(['Hello World, How are you!', 'hi there.']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKanoDukoowD",
        "outputId": "4169b773-f941-4056-bd86-12da91ce2ea2"
      },
      "id": "EKanoDukoowD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 232k/232k [00:00<00:00, 5.42MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ['7592', '2088', '1010', '2129', '2024', '2017', '999']\n",
            "[['7592', '2088', '1010', '2129', '2024', '2017', '999'], ['7632', '2045', '1012']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_BERT = T.BERTTokenizer(vocab_path=VOCAB_FILE,\n",
        "                            do_lower_case=True,\n",
        "                            return_tokens=True)\n",
        "\n",
        "print(tokenizer_BERT('Hello World, How are you!'))\n",
        "print(tokenizer_BERT(['Hello World, How are you!', 'hi there.']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAhHyl5-fesA",
        "outputId": "cf827290-e575-4f67-e884-7fb798d4da34"
      },
      "id": "xAhHyl5-fesA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 232k/232k [00:00<00:00, 4.61MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', ',', 'how', 'are', 'you', '!']\n",
            "[['hello', 'world', ',', 'how', 'are', 'you', '!'], ['hi', 'there', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ToTensor"
      ],
      "metadata": {
        "id": "n2w3iZc_oVWE"
      },
      "id": "n2w3iZc_oVWE"
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = 'hi there.'\n",
        "sent2 = 'Hello World, How are you!'"
      ],
      "metadata": {
        "id": "v4Pc1gpwoYd2"
      },
      "id": "v4Pc1gpwoYd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer_BERT([sent1, sent2])\n",
        "\n",
        "token_ids1 = [vectorization.stoi[token] for token in tokens[0]]\n",
        "token_ids2 = [vectorization.stoi[token] for token in tokens[1]]\n",
        "token_ids = [token_ids1, token_ids2]\n",
        "token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XONOGr_go4WI",
        "outputId": "3c77955e-ea5b-4598-fbdf-febeeb7b097b"
      },
      "id": "XONOGr_go4WI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[11083, 63, 2], [13075, 85, 1, 197, 32, 81, 805]]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_tensor = T.ToTensor(padding_value=0)\n",
        "to_tensor(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mwlGzTrpp5-",
        "outputId": "27d68b1f-0490-4d62-ce8d-abbea03e452b"
      },
      "id": "8mwlGzTrpp5-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[11083,    63,     2,     0,     0,     0,     0],\n",
              "        [13075,    85,     1,   197,    32,    81,   805]])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### Truncate"
      ],
      "metadata": {
        "id": "ZXCYDBD0prSd"
      },
      "id": "ZXCYDBD0prSd"
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens[1])\n",
        "T.Truncate(max_seq_len=3)(tokens[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLzm1QYlptGD",
        "outputId": "8b197d67-1bb0-47ef-a020-b02a5f016770"
      },
      "id": "gLzm1QYlptGD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', ',', 'how', 'are', 'you', '!']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world', ',']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### Sequential"
      ],
      "metadata": {
        "id": "eT0NLMI1pw8l"
      },
      "id": "eT0NLMI1pw8l"
    },
    {
      "cell_type": "code",
      "source": [
        "tr = T.Sequential(T.BERTTokenizer(vocab_path=VOCAB_FILE,\n",
        "                            do_lower_case=True,\n",
        "                            return_tokens=True),\n",
        "                  T.Truncate(max_seq_len=3))\n",
        "tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAfhVPr1py_w",
        "outputId": "6dab6abf-19f6-4c95-f470-0be2e1b8173e"
      },
      "id": "oAfhVPr1py_w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 232k/232k [00:00<00:00, 5.49MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): BERTTokenizer()\n",
              "  (1): Truncate()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent2)\n",
        "tr(sent2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWnPCT7fp0IP",
        "outputId": "e3db0740-e933-4951-89e9-3dd525f1965b"
      },
      "id": "GWnPCT7fp0IP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World, How are you!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world', ',']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Utils - Ngrams"
      ],
      "metadata": {
        "id": "Kvd-V8lWqQq2"
      },
      "id": "Kvd-V8lWqQq2"
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import ngrams_iterator"
      ],
      "metadata": {
        "id": "JkuZjcNtqS16"
      },
      "id": "JkuZjcNtqS16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent3 = 'Return an iterator that yields the given tokens and their ngrams.'\n",
        "tokens = tokenizer_BERT(sent3)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "pT62a5lxrs0f",
        "outputId": "62ae01b2-6f07-4fd4-bd32-5623d6424e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pT62a5lxrs0f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['return', 'an', 'it', '##era', '##tor', 'that', 'yields', 'the', 'given', 'token', '##s', 'and', 'their', 'ng', '##ram', '##s', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent3 = 'Return an iterator that yields the given tokens and their ngrams.'\n",
        "tokens = tokenizer(sent3)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "hNINwDcZrrKN",
        "outputId": "a99b46e5-273d-45bf-bebe-273063d6cc41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hNINwDcZrrKN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['return', 'an', 'iterator', 'that', 'yields', 'the', 'given', 'tokens', 'and', 'their', 'ngrams', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams_iterator(tokens, ngrams=3)"
      ],
      "metadata": {
        "id": "AMXSQGi5r02x",
        "outputId": "89ae8979-8f41-4647-a26b-2546c3241c11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AMXSQGi5r02x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object ngrams_iterator at 0x7a671c61f740>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(ngrams_iterator(tokens, ngrams=3))"
      ],
      "metadata": {
        "id": "bv8BQmPYrxgc",
        "outputId": "ca01ca91-3a7f-4bcf-e20e-f229fa04d699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bv8BQmPYrxgc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['return',\n",
              " 'an',\n",
              " 'iterator',\n",
              " 'that',\n",
              " 'yields',\n",
              " 'the',\n",
              " 'given',\n",
              " 'tokens',\n",
              " 'and',\n",
              " 'their',\n",
              " 'ngrams',\n",
              " '.',\n",
              " 'return an',\n",
              " 'an iterator',\n",
              " 'iterator that',\n",
              " 'that yields',\n",
              " 'yields the',\n",
              " 'the given',\n",
              " 'given tokens',\n",
              " 'tokens and',\n",
              " 'and their',\n",
              " 'their ngrams',\n",
              " 'ngrams .',\n",
              " 'return an iterator',\n",
              " 'an iterator that',\n",
              " 'iterator that yields',\n",
              " 'that yields the',\n",
              " 'yields the given',\n",
              " 'the given tokens',\n",
              " 'given tokens and',\n",
              " 'tokens and their',\n",
              " 'and their ngrams',\n",
              " 'their ngrams .']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PytorchCuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}